<!DOCTYPE html>
<html lang="en-us">
<head>
  <link rel="preload" href="/website/lib/font-awesome/webfonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/website/lib/font-awesome/webfonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/website/lib/font-awesome/webfonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/website/lib/JetBrainsMono/web/woff2/JetBrainsMono-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <script type="text/javascript" src="https://latest.cactus.chat/cactus.js"></script>
  <link rel="stylesheet" href="https://latest.cactus.chat/style.css" type="text/css">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title> A Weekend on Neural Network Interpretability | Auguste&#39;s website</title>
  <link rel = 'canonical' href = 'https://augustebaum.github.io/website/posts/2023-05-29-03-a-weekend-on-neural-network-interpretability/'>
  <meta name="description" content="My name is Auguste Baum. In case you were wondering, my first name is pronounced like the eighth month of the year. I have just graduated with a Master&#39;s degree in Data Science at EPFL, in Switzerland. I have also been involved with [Resilio](https://www.resilio.tech), a start-up aiming to help companies reduce their environmental impact.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta property="og:title" content="A Weekend on Neural Network Interpretability" />
<meta property="og:description" content="This is a summary of my experience of the weekend on interpretability I spent from 2023-05-27 morning to 2023-05-28 evening. This was organized in Paris by EffiSciences.
Saturday Morning We got an introduction to AI risk and the idea of interpretability was introduced as a possible way to counter Goal Misgeneralization, whereby an AI can end up pursuing a goal that is different from the one that was intended, because it found a proxy to the &ldquo;right solution&rdquo;." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://augustebaum.github.io/website/posts/2023-05-29-03-a-weekend-on-neural-network-interpretability/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-29T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-05-29T00:00:00+00:00" />

  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="A Weekend on Neural Network Interpretability"/>
<meta name="twitter:description" content="This is a summary of my experience of the weekend on interpretability I spent from 2023-05-27 morning to 2023-05-28 evening. This was organized in Paris by EffiSciences.
Saturday Morning We got an introduction to AI risk and the idea of interpretability was introduced as a possible way to counter Goal Misgeneralization, whereby an AI can end up pursuing a goal that is different from the one that was intended, because it found a proxy to the &ldquo;right solution&rdquo;."/>

  
  
    
  
  
  <link rel="stylesheet" href="https://augustebaum.github.io/website/css/styles.94f653e9e151e28067a7c5dbbc4600cbd5a3c721e79faaf971e523c40f3b249b8e4f20bb57810dfffa8d559ca5c140fd56eb4cd9c0853113ad08e66afdb08bdd.css" integrity="sha512-lPZT6eFR4oBnp8XbvEYAy9WjxyHnn6r5ceUjxA87JJuOTyC7V4EN//qNVZylwUD9VutM2cCFMROtCOZq/bCL3Q=="> 

  
  
  
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
<link rel="icon" type="image/png" href="https://augustebaum.github.io/website/images/favicon.ico" />

  
  
  
  
</head>

<body class="max-width mx-auto px3 ltr">
  <div class="content index py4">

  <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;" aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
        <li><a href="/">Home</a></li>
         
        <li><a href="/posts">Writings</a></li>
         
        <li><a href="/tags">Tags</a></li>
         
        <li><a href="/about">About</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li>
          <a class="icon" href=" https://augustebaum.github.io/website/posts/2023-05-19-02-nix-for-python-attempt-no.-2/" aria-label="Previous">
            <i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i>
          </a>
        </li>
        
        
        <li>
          <a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" aria-label="Top of Page">
            <i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i>
          </a>
        </li>
        <li>
          <a class="icon" href="#" aria-label="Share">
            <i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i>
          </a>
        </li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      
      <ul>
  
  
    
  
  
  <li>
    <a class="icon" href="http://www.facebook.com/sharer.php?u=https%3a%2f%2faugustebaum.github.io%2fwebsite%2fposts%2f2023-05-29-03-a-weekend-on-neural-network-interpretability%2f" aria-label="Facebook">
      <i class="fab fa-facebook " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://twitter.com/share?url=https%3a%2f%2faugustebaum.github.io%2fwebsite%2fposts%2f2023-05-29-03-a-weekend-on-neural-network-interpretability%2f&text=A%20Weekend%20on%20Neural%20Network%20Interpretability" aria-label="Twitter">
      <i class="fab fa-twitter " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2faugustebaum.github.io%2fwebsite%2fposts%2f2023-05-29-03-a-weekend-on-neural-network-interpretability%2f&title=A%20Weekend%20on%20Neural%20Network%20Interpretability" aria-label="Linkedin">
      <i class="fab fa-linkedin " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2faugustebaum.github.io%2fwebsite%2fposts%2f2023-05-29-03-a-weekend-on-neural-network-interpretability%2f&is_video=false&description=A%20Weekend%20on%20Neural%20Network%20Interpretability" aria-label="Pinterest">
      <i class="fab fa-pinterest " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="mailto:?subject=A%20Weekend%20on%20Neural%20Network%20Interpretability&body=Check out this article: https%3a%2f%2faugustebaum.github.io%2fwebsite%2fposts%2f2023-05-29-03-a-weekend-on-neural-network-interpretability%2f" aria-label="Email">
      <i class="fas fa-envelope " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://getpocket.com/save?url=https%3a%2f%2faugustebaum.github.io%2fwebsite%2fposts%2f2023-05-29-03-a-weekend-on-neural-network-interpretability%2f&title=A%20Weekend%20on%20Neural%20Network%20Interpretability" aria-label="Pocket">
      <i class="fab fa-get-pocket " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://reddit.com/submit?url=https%3a%2f%2faugustebaum.github.io%2fwebsite%2fposts%2f2023-05-29-03-a-weekend-on-neural-network-interpretability%2f&title=A%20Weekend%20on%20Neural%20Network%20Interpretability" aria-label="reddit">
      <i class="fab fa-reddit " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.tumblr.com/share/link?url=https%3a%2f%2faugustebaum.github.io%2fwebsite%2fposts%2f2023-05-29-03-a-weekend-on-neural-network-interpretability%2f&name=A%20Weekend%20on%20Neural%20Network%20Interpretability&description=This%20is%20a%20summary%20of%20my%20experience%20of%20the%20weekend%20on%20interpretability%20I%20spent%20from%202023-05-27%20morning%20to%202023-05-28%20evening.%20This%20was%20organized%20in%20Paris%20by%20EffiSciences.%0aSaturday%20Morning%20We%20got%20an%20introduction%20to%20AI%20risk%20and%20the%20idea%20of%20interpretability%20was%20introduced%20as%20a%20possible%20way%20to%20counter%20Goal%20Misgeneralization%2c%20whereby%20an%20AI%20can%20end%20up%20pursuing%20a%20goal%20that%20is%20different%20from%20the%20one%20that%20was%20intended%2c%20because%20it%20found%20a%20proxy%20to%20the%20%26ldquo%3bright%20solution%26rdquo%3b." aria-label="Tumblr">
      <i class="fab fa-tumblr " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2faugustebaum.github.io%2fwebsite%2fposts%2f2023-05-29-03-a-weekend-on-neural-network-interpretability%2f&t=A%20Weekend%20on%20Neural%20Network%20Interpretability" aria-label="Hacker News">
      <i class="fab fa-hacker-news " aria-hidden="true"></i>
    </a>
  </li>
</ul>

    </div>
    
    <div id="toc">
      <nav id="TableOfContents">
  <ul>
    <li><a href="#saturday-morning">Saturday Morning</a></li>
    <li><a href="#saturday-afternoon">Saturday Afternoon</a></li>
    <li><a href="#sunday-morning">Sunday Morning</a></li>
    <li><a href="#sunday-afternoon">Sunday Afternoon</a></li>
    <li><a href="#takeaway">Takeaway</a>
      <ul>
        <li><a href="#further-readings">Further Readings</a>
          <ul>
            <li><a href="#seri-mats-streamshttpswwwserimatsorg"><a href="https://www.serimats.org/">SERI MATS streams</a></a></li>
          </ul>
        </li>
        <li><a href="#questions">Questions</a></li>
        <li><a href="#final-thoughts">Final Thoughts</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
    
  </span>
</div>


  <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
    <header>
      <h1 class="posttitle" itemprop="name headline">
        A Weekend on Neural Network Interpretability
      </h1>
      <div class="meta">
        
        <div class="postdate">
          
          <time datetime="2023-05-29 00:00:00 &#43;0000 UTC" itemprop="datePublished">2023-05-29</time>
          
        </div>
        
        
        <div class="article-read-time">
          <i class="far fa-clock"></i>
          
          3 minute read
        </div>
        
        
        
      </div>
    </header>

  
    
    <div class="content" itemprop="articleBody">
      <p>This is a summary of my experience of <a href="https://www.eventbrite.fr/e/billets-week-end-de-formation-intensive-en-interpretabilite-601236714197">the weekend on interpretability</a> I spent from 2023-05-27 morning to 2023-05-28 evening.
This was organized in Paris by <a href="https://www.effisciences.org/">EffiSciences</a>.</p>
<h2 id="saturday-morning">Saturday Morning</h2>
<p>We got an introduction to AI risk and the idea of interpretability was introduced as a possible way to counter <a href="https://deepmindsafetyresearch.medium.com/goal-misgeneralisation-why-correct-specifications-arent-enough-for-correct-goals-cf96ebc60924">Goal Misgeneralization</a>, whereby an AI can end up pursuing a goal that is different from the one that was intended, because it found a proxy to the &ldquo;right solution&rdquo;. This implies that if the proxy behaves differently in deployment than in training, the agent will still just follow the proxy even if this yields bad outcomes.</p>
<p>Interpretability means figuring out what is going on inside a neural network in terms of what information it retains as relevant to taking a decision. For example, In <a href="https://distill.pub/2017/feature-visualization/"><em>Feature visualization</em></a> the authors present a method to get an example of an image that highly <em>excites</em> a particular neuron; this supposedly tells us what human-intelligible concept this neuron &ldquo;recognizes&rdquo;. This is reminiscent of the <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6822296/">well-critiqued</a> &ldquo;grandmother neuron&rdquo; theory of representation of concepts in the human brain.</p>
<p>Then we went into a particular interpretability technique named <a href="https://ieeexplore.ieee.org/document/8237336">GradCam</a>. This method allows you to see which pixels are relevant to a particular input-output (e.g. image-class) pair.</p>
<p>In a workshop, we filled out a notebook implementing GradCam. In particular, I was made aware of the Einstein summation notation, implemented in the form of the <a href="https://einops.rocks/"><code>einops</code> Python package</a> which is an elegant way to express complex linear tensor operations in few symbols.</p>
<p>According to <a href="https://dl.acm.org/doi/10.5555/3327546.3327621">Adebayo <em>et al.</em></a>, GradCam is more reliable than other feature importance methods for images; the other methods seem closer to edge detectors.</p>
<hr>
<h2 id="saturday-afternoon">Saturday Afternoon</h2>
<p>We got a quick introduction to Transformers as a core part of the architecture of many current state-of-the-art models. As my former internship supervisor said, AI practitioners tend to use it a lot, sometimes excessively:</p>
<p><img src="https://i.imgflip.com/7nj36t.jpg" alt="Flex-tape meme: AI engineers slap Transformers onto literally every task they come across."></p>
<p>We covered the attention block architecture, discussed the concept of residual stream and positional embedding. By that point in the day my brain was fried so things like the terminology of &ldquo;queries&rdquo;, &ldquo;keys&rdquo; and &ldquo;values&rdquo; used when describing the attention mechanism felt a bit esoteric.</p>
<hr>
<h2 id="sunday-morning">Sunday Morning</h2>
<p>We went over Transformers once more, then we were explained the <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens">LogitLens</a> method. It&rsquo;s much simpler than to explain GradCam but I&rsquo;m less sure that it&rsquo;s really believable. We spent the morning implementing it (by adding hooks onto a Hugging Face pre-trained GPT-2 model).</p>
<hr>
<h2 id="sunday-afternoon">Sunday Afternoon</h2>
<p>We went over many (I&rsquo;d say too many) papers on interpreting Transformers. Once again was brain was fried pretty quickly and my knowledge of transformers was too fresh to really get anything of what was going on. However, I do understand that attention blocks <em>seem</em> interpretable and there are some connections with linear algebra (e.g. the query-keys product is invariant by rotation of the two matrices, and something about working only in small subspaces of some big space).</p>
<p><a href="https://www.lesswrong.com/posts/TvrfY4c9eaGLeyDkE/induction-heads-illustrated">This illustration</a> is considered not bad, although you still have to ponder on it for a while.</p>
<p>We discussed the concern that interpretability itself is likely not a panacea but rather a nice, &ldquo;cool&rdquo; subject that can help to introduce ML practitioners to AI safety research.</p>
<hr>
<h2 id="takeaway">Takeaway</h2>
<h3 id="further-readings">Further Readings</h3>
<ul>
<li>The same author wrote <a href="https://www.lesswrong.com/posts/euam65XjigaCJQkcN/an-analogy-for-understanding-transformers"><em>An Analogy for Understanding Transformers</em></a> very recently. He recommends having been through <a href="https://www.youtube.com/watch?v=bOYE6E8JrtU">Neel Nanda&rsquo;s tutorial on the subject</a>.</li>
<li>The <a href="https://course.mlsafety.org/">Center for AI Safety introduction course</a> which I intend to do already&hellip;</li>
</ul>
<h4 id="seri-mats-streamshttpswwwserimatsorg"><a href="https://www.serimats.org/">SERI MATS streams</a></h4>
<ul>
<li>Aligning Language Models</li>
<li>Agent Foundations</li>
<li>Consequentialist Cognition and Deep Constraints</li>
<li>Cyborgism</li>
<li>Deceptive AI</li>
<li>Evaluating Dangerous Capabilities</li>
<li>Interdisciplinary AI Safety</li>
<li>Mechanistic Interpretability</li>
<li>Multipolar AI Safety</li>
<li>Powerseeking in Language Models</li>
<li>Shard Theory</li>
<li>Understanding AI Hacking</li>
</ul>
<h3 id="questions">Questions</h3>
<ul>
<li>Is AI really that dangerous? As a student having taken courses on cryptography and privacy, I always find it surprisingly difficult to properly argue for the importance of privacy and safety with peers and practitioners who might be simply oblivious to these issues. The <a href="https://www.agisafetyfundamentals.com/ai-alignment-curriculum">AGI safety fundamentals</a> could help give me arguments there?</li>
<li>What is &ldquo;grokking&rdquo;?</li>
</ul>
<h3 id="final-thoughts">Final Thoughts</h3>
<p>This was my first &ldquo;formal&rdquo; introduction to AI safety apart from hearing people talk about it around me and watching <a href="https://www.youtube.com/c/robertmilesai">Rob Miles videos</a>. Although the possibility of the future of humanity is supposedly at stake, my goal was just to get into some technical details. In the meantime I got to actually talk with very smart people that have been thinking about these issues for much longer than I have.</p>
<p>This is post number 003 of <a href="https://100daystooffload.com/">#100daystooffload</a>.</p>

    </div>
  </article>

  
  






  <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/posts">Writings</a></li>
         
          <li><a href="/tags">Tags</a></li>
         
          <li><a href="/about">About</a></li>
        
      </ul>
    </div>

    
    <div id="toc-footer" style="display: none">
      <nav id="TableOfContents">
  <ul>
    <li><a href="#saturday-morning">Saturday Morning</a></li>
    <li><a href="#saturday-afternoon">Saturday Afternoon</a></li>
    <li><a href="#sunday-morning">Sunday Morning</a></li>
    <li><a href="#sunday-afternoon">Sunday Afternoon</a></li>
    <li><a href="#takeaway">Takeaway</a>
      <ul>
        <li><a href="#further-readings">Further Readings</a>
          <ul>
            <li><a href="#seri-mats-streamshttpswwwserimatsorg"><a href="https://www.serimats.org/">SERI MATS streams</a></a></li>
          </ul>
        </li>
        <li><a href="#questions">Questions</a></li>
        <li><a href="#final-thoughts">Final Thoughts</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
    

    <div id="share-footer" style="display: none">
      
      <ul>
  
  
    
  
  
  <li>
    <a class="icon" href="http://www.facebook.com/sharer.php?u=https%3a%2f%2faugustebaum.github.io%2fwebsite%2fposts%2f2023-05-29-03-a-weekend-on-neural-network-interpretability%2f" aria-label="Facebook">
      <i class="fab fa-facebook fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://twitter.com/share?url=https%3a%2f%2faugustebaum.github.io%2fwebsite%2fposts%2f2023-05-29-03-a-weekend-on-neural-network-interpretability%2f&text=A%20Weekend%20on%20Neural%20Network%20Interpretability" aria-label="Twitter">
      <i class="fab fa-twitter fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2faugustebaum.github.io%2fwebsite%2fposts%2f2023-05-29-03-a-weekend-on-neural-network-interpretability%2f&title=A%20Weekend%20on%20Neural%20Network%20Interpretability" aria-label="Linkedin">
      <i class="fab fa-linkedin fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2faugustebaum.github.io%2fwebsite%2fposts%2f2023-05-29-03-a-weekend-on-neural-network-interpretability%2f&is_video=false&description=A%20Weekend%20on%20Neural%20Network%20Interpretability" aria-label="Pinterest">
      <i class="fab fa-pinterest fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="mailto:?subject=A%20Weekend%20on%20Neural%20Network%20Interpretability&body=Check out this article: https%3a%2f%2faugustebaum.github.io%2fwebsite%2fposts%2f2023-05-29-03-a-weekend-on-neural-network-interpretability%2f" aria-label="Email">
      <i class="fas fa-envelope fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://getpocket.com/save?url=https%3a%2f%2faugustebaum.github.io%2fwebsite%2fposts%2f2023-05-29-03-a-weekend-on-neural-network-interpretability%2f&title=A%20Weekend%20on%20Neural%20Network%20Interpretability" aria-label="Pocket">
      <i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://reddit.com/submit?url=https%3a%2f%2faugustebaum.github.io%2fwebsite%2fposts%2f2023-05-29-03-a-weekend-on-neural-network-interpretability%2f&title=A%20Weekend%20on%20Neural%20Network%20Interpretability" aria-label="reddit">
      <i class="fab fa-reddit fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.tumblr.com/share/link?url=https%3a%2f%2faugustebaum.github.io%2fwebsite%2fposts%2f2023-05-29-03-a-weekend-on-neural-network-interpretability%2f&name=A%20Weekend%20on%20Neural%20Network%20Interpretability&description=This%20is%20a%20summary%20of%20my%20experience%20of%20the%20weekend%20on%20interpretability%20I%20spent%20from%202023-05-27%20morning%20to%202023-05-28%20evening.%20This%20was%20organized%20in%20Paris%20by%20EffiSciences.%0aSaturday%20Morning%20We%20got%20an%20introduction%20to%20AI%20risk%20and%20the%20idea%20of%20interpretability%20was%20introduced%20as%20a%20possible%20way%20to%20counter%20Goal%20Misgeneralization%2c%20whereby%20an%20AI%20can%20end%20up%20pursuing%20a%20goal%20that%20is%20different%20from%20the%20one%20that%20was%20intended%2c%20because%20it%20found%20a%20proxy%20to%20the%20%26ldquo%3bright%20solution%26rdquo%3b." aria-label="Tumblr">
      <i class="fab fa-tumblr fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2faugustebaum.github.io%2fwebsite%2fposts%2f2023-05-29-03-a-weekend-on-neural-network-interpretability%2f&t=A%20Weekend%20on%20Neural%20Network%20Interpretability" aria-label="Hacker News">
      <i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i>
    </a>
  </li>
</ul>

    </div>

    <div id="actions-footer">
      
        <a id="menu-toggle" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;" aria-label="Menu">
          <i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
        <a id="toc-toggle" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;" aria-label="TOC">
          <i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share-toggle" class="icon" href="#" onclick="$('#share-footer').toggle();return false;" aria-label="Share">
          <i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" aria-label="Top of Page">
          <i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>


  <footer id="footer">
  <div class="footer-left">
    Copyright  &copy; 2023  Auguste Baum 
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
        <li><a href="/">Home</a></li>
         
        <li><a href="/posts">Writings</a></li>
         
        <li><a href="/tags">Tags</a></li>
         
        <li><a href="/about">About</a></li>
        
      </ul>
    </nav>
  </div>
</footer>


  </div>
</body>

<link rel="stylesheet" href=/lib/font-awesome/css/all.min.css>
<script src=/lib/jquery/jquery.min.js></script>
<script src=/js/main.js></script>

<script src=/website/js/code-copy.js></script>




</html>
